{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ffc755cc",
   "metadata": {},
   "source": [
    "HACKATON; CENICAFE \n",
    "\n",
    "CAFETERITOS \n",
    "\n",
    "üß™ An√°lisis de Datos Eddy Covariance para Modelado de NEE\n",
    "\n",
    "Objetivo: Entrenar un modelo de machine learning para estimar el Intercambio Neto de CO‚ÇÇ (NEE) en base a variables meteorol√≥gicas y de teledetecci√≥n, usando datos de torre Eddy Covariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe272a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# MODELO DE MACHINE LEARNING PARA NEE\n",
    "# ====================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar paquetes necesarios\n",
    "%pip install pandas matplotlib seaborn scikit-learn xgboost scipy statsmodels geemap\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f3b362",
   "metadata": {},
   "source": [
    "1. üìÇ Carga y limpieza de datos\n",
    "\n",
    "Se cargan los datos desde un archivo .csv separado por tabulaciones (\\t). Los valores faltantes vienen codificados como -9999, por lo que se reemplazan por NaN para su correcto tratamiento.\n",
    "\n",
    "Tambi√©n se convierte la columna TIMESTAMP_START al formato de fecha y hora est√°ndar (datetime) para facilitar el filtrado temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2eb72d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1. Cargar los datos\n",
    "# ---------------------------\n",
    "archivo = \"AMF_CR-Fsc_BASE_HH_2-5.csv\"  # ‚Üê CAMBIA este nombre por el archivo real\n",
    "\n",
    "# Leer el archivo CSV eliminando las primeras 2 filas de metadatos\n",
    "df = pd.read_csv(archivo, sep=\",\", skiprows=2, low_memory=False)\n",
    "\n",
    "print(f\"‚úÖ Archivo le√≠do exitosamente\")\n",
    "\n",
    "print(f\"üìä Filas: {len(df)}\")\n",
    "print(f\"üìã Columnas: {len(df.columns)}\")\n",
    "print(f\"üìã Primeras 5 columnas: {list(df.columns[:5])}\")\n",
    "print(f\"\\nüìã Primeras 3 filas:\")\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ebdca",
   "metadata": {},
   "source": [
    "2. ‚è≥ Filtro temporal: desde 9 de diciembre de 2016 a las 10:30\n",
    "\n",
    "Los datos de teledetecci√≥n (NDVI, PRI) solo est√°n disponibles desde finales de 2016. Por tanto, se filtran los registros para trabajar solo con los datos a partir del 9 de diciembre de 2016 a las 10:30 am.\n",
    "\n",
    "3. üïí Extracci√≥n de variables temporales\n",
    "\n",
    "Se derivan columnas auxiliares a partir del timestamp para capturar la estacionalidad (doy: d√≠a del a√±o) y el ciclo diurno (hour: hora decimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887dc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 2. Limpieza inicial\n",
    "# ---------------------------\n",
    "df.replace(-9999, np.nan, inplace=True)\n",
    "df['TIMESTAMP_START'] = pd.to_datetime(df['TIMESTAMP_START'], format=\"%Y%m%d%H%M\")\n",
    "\n",
    "# ---------------------------\n",
    "# 3. Filtro desde NDVI no NaN\n",
    "# ---------------------------\n",
    "# Filtrar registros donde NDVI no sea NaN\n",
    "df = df[df['NDVI'].notna()].reset_index(drop=True)\n",
    "\n",
    "print(\"Tama√±o de la base tras filtro NDVI no NaN:\", len(df))\n",
    "print(\"Periodo de la base de datos despu√©s del filtro NDVI no NaN:\")\n",
    "print(\"Inicio:\", df['TIMESTAMP_START'].min())\n",
    "print(\"Fin   :\", df['TIMESTAMP_START'].max())\n",
    "\n",
    "# Analizar huecos temporales en la serie cada 30 minutos\n",
    "df_sorted = df.sort_values('TIMESTAMP_START').reset_index(drop=True)\n",
    "intervalo_esperado = pd.Timedelta(minutes=30)\n",
    "diferencias = df_sorted['TIMESTAMP_START'].diff()\n",
    "huecos = df_sorted[diferencias > intervalo_esperado]\n",
    "\n",
    "print(f\"Total de huecos encontrados: {len(huecos)}\")\n",
    "if not huecos.empty:\n",
    "    print(\"Primeros 5 huecos:\")\n",
    "    for idx in huecos.index[:5]:\n",
    "        anterior = df_sorted.loc[idx - 1, 'TIMESTAMP_START']\n",
    "        actual = df_sorted.loc[idx, 'TIMESTAMP_START']\n",
    "        print(f\"Hueco entre {anterior} y {actual} (diferencia: {actual - anterior})\")\n",
    "else:\n",
    "    print(\"No se encontraron huecos temporales mayores a 30 minutos.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c3759",
   "metadata": {},
   "source": [
    "4. üî¢ Selecci√≥n de variables predictoras y objetivo\n",
    "\n",
    "A continuaci√≥n se listan las variables seleccionadas como entradas del modelo y su variable objetivo. Se incluye su unidad y una breve descripci√≥n: \n",
    "\n",
    "| Variable       | Unidad              | Descripci√≥n                                                                 |\n",
    "|----------------|----------------------|-----------------------------------------------------------------------------|\n",
    "| `TA_1_1_1`     | ¬∞C                   | Temperatura del aire (sensor a 1.1 m)                                       |\n",
    "| `RH_1_1_1`     | %                    | Humedad relativa (sensor a 1.1 m)                                           |\n",
    "| `VPD_PI`       | hPa                  | D√©ficit de presi√≥n de vapor                                                |\n",
    "| `SW_IN_1_1_1`  | W m‚Åª¬≤                | Radiaci√≥n solar de onda corta entrante                                     |\n",
    "| `LW_IN`        | W m‚Åª¬≤                | Radiaci√≥n de onda larga entrante                                           |\n",
    "| `P`            | mm                   | Precipitaci√≥n acumulada                                                    |\n",
    "| `WS`           | m s‚Åª¬π                | Velocidad del viento                                                       |\n",
    "| `NDVI`         | adimensional (0‚Äì1)   | √çndice de vegetaci√≥n normalizado (NDVI)                                    |\n",
    "| `PRI`          | adimensional         | √çndice de reflectancia fotosint√©tica (Photochemical Reflectance Index)     |\n",
    "| `doy`          | D√≠a del a√±o (1‚Äì366)  | D√≠a del a√±o para capturar estacionalidad                                   |\n",
    "| `hour`         | Hora decimal (0‚Äì24)  | Hora decimal del d√≠a para capturar el ciclo diurno                         |\n",
    "| `NEE_PI_F`     | toneladas CO‚ÇÇ/ha     | **Intercambio Neto de Carbono (variable objetivo a predecir)**             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4e186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 4. Variables temporales √∫tiles\n",
    "# ---------------------------\n",
    "df['year'] = df['TIMESTAMP_START'].dt.year\n",
    "df['month'] = df['TIMESTAMP_START'].dt.month\n",
    "df['doy'] = df['TIMESTAMP_START'].dt.dayofyear\n",
    "df['hour'] = df['TIMESTAMP_START'].dt.hour + df['TIMESTAMP_START'].dt.minute / 60\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9103bd4",
   "metadata": {},
   "source": [
    "5. üìä An√°lisis exploratorio de variables\n",
    "\n",
    "Se presentan dos tipos de an√°lisis:\n",
    "\t‚Ä¢\tEstad√≠sticas descriptivas de las variables predictoras (media, desviaci√≥n, m√≠nimo, m√°ximo, etc.).\n",
    "\t‚Ä¢\tCorrelaci√≥n entre todas las variables, enfoc√°ndose en la relaci√≥n entre las entradas y el NEE (NEE_PI_F), usando un mapa de calor (heatmap).\n",
    "\n",
    "Este an√°lisis permite identificar relaciones clave, redundancias y la importancia potencial de cada variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1403a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 5. Variables seleccionadas\n",
    "# ---------------------------\n",
    "variables_entrada = [\n",
    "    'TA_1_1_1', 'RH_1_1_1', 'VPD_PI',\n",
    "    'SW_IN_1_1_1', 'LW_IN', 'P', 'WS',\n",
    "    'NDVI', 'PRI', 'doy', 'hour'\n",
    "]\n",
    "variable_objetivo = 'NEE_PI_F'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0816cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# AN√ÅLISIS PROFUNDO DE VARIABLES\n",
    "# ========================================\n",
    "\n",
    "# Filtrar datos v√°lidos para el an√°lisis\n",
    "df_analisis = df.dropna(subset=variables_entrada + [variable_objetivo])\n",
    "\n",
    "print(f\"üìä RESUMEN GENERAL DEL DATASET\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total de registros v√°lidos: {len(df_analisis):,}\")\n",
    "print(f\"Per√≠odo de datos: {df_analisis['TIMESTAMP_START'].min()} - {df_analisis['TIMESTAMP_START'].max()}\")\n",
    "print(f\"A√±os disponibles: {sorted(df_analisis['year'].unique())}\")\n",
    "print(f\"Registros por a√±o: {df_analisis['year'].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 24))\n",
    "fig.suptitle('üìà AN√ÅLISIS TEMPORAL DE VARIABLES DE ENTRADA Y OBJETIVO', fontsize=16, fontweight='bold')\n",
    "\n",
    "# An√°lisis de cada variable\n",
    "for i, var in enumerate(variables_entrada + [variable_objetivo]):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Serie temporal mensual\n",
    "    monthly_data = df_analisis.groupby(df_analisis['TIMESTAMP_START'].dt.to_period('M'))[var].mean()\n",
    "    monthly_data.plot(ax=ax, linewidth=2, color='steelblue')\n",
    "    \n",
    "    ax.set_title(f'{var}', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Tiempo')\n",
    "    if var == variable_objetivo:\n",
    "        ax.set_ylabel('toneladas CO‚ÇÇ/ha')\n",
    "    else:\n",
    "        ax.set_ylabel('Valor')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis estad√≠stico detallado\n",
    "print(f\"\\nüìä ESTAD√çSTICAS DESCRIPTIVAS DETALLADAS\")\n",
    "print(f\"{'='*60}\")\n",
    "stats = df_analisis[variables_entrada + [variable_objetivo]].describe()\n",
    "print(stats.round(3))\n",
    "\n",
    "# An√°lisis de valores faltantes\n",
    "print(f\"\\nüîç AN√ÅLISIS DE VALORES FALTANTES\")\n",
    "print(f\"{'='*40}\")\n",
    "missing_data = df[variables_entrada + [variable_objetivo]].isnull().sum()\n",
    "missing_pct = (missing_data / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Variable': missing_data.index,\n",
    "    'Valores_Faltantes': missing_data.values,\n",
    "    'Porcentaje': missing_pct.values\n",
    "}).sort_values('Porcentaje', ascending=False)\n",
    "print(missing_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc0ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# AN√ÅLISIS DE PATRONES TEMPORALES\n",
    "# ========================================\n",
    "\n",
    "# Crear figura para an√°lisis de patrones temporales\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('‚è∞ PATRONES TEMPORALES DE NEE Y VARIABLES CLAVE', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Patr√≥n diario promedio del NEE\n",
    "ax1 = axes[0, 0]\n",
    "daily_pattern = df_analisis.groupby('hour')[variable_objetivo].mean()\n",
    "daily_pattern.plot(kind='line', ax=ax1, color='darkgreen', linewidth=3)\n",
    "ax1.set_title('üìÖ Patr√≥n Diario Promedio del NEE', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Hora del d√≠a')\n",
    "ax1.set_ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 2. Patr√≥n estacional del NEE\n",
    "ax2 = axes[0, 1]\n",
    "seasonal_pattern = df_analisis.groupby('doy')[variable_objetivo].mean()\n",
    "seasonal_pattern.plot(kind='line', ax=ax2, color='darkblue', linewidth=2)\n",
    "ax2.set_title('üå± Patr√≥n Estacional del NEE', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('D√≠a del a√±o')\n",
    "ax2.set_ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "# 3. Relaci√≥n NEE vs Radiaci√≥n Solar por hora\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(df_analisis['SW_IN_1_1_1'], df_analisis[variable_objetivo], \n",
    "                     c=df_analisis['hour'], cmap='viridis', alpha=0.6)\n",
    "ax3.set_title('‚òÄÔ∏è NEE vs Radiaci√≥n Solar (coloreado por hora)', fontsize=12, fontweight='bold')\n",
    "ax3.set_xlabel('Radiaci√≥n Solar (W m‚Åª¬≤)')\n",
    "ax3.set_ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "plt.colorbar(scatter, ax=ax3, label='Hora del d√≠a')\n",
    "\n",
    "# 4. Relaci√≥n NEE vs Temperatura por estaci√≥n\n",
    "ax4 = axes[1, 1]\n",
    "# Definir estaciones\n",
    "df_analisis['season'] = df_analisis['month'].map({\n",
    "    12: 'Verano', 1: 'Verano', 2: 'Verano',\n",
    "    3: 'Oto√±o', 4: 'Oto√±o', 5: 'Oto√±o',\n",
    "    6: 'Invierno', 7: 'Invierno', 8: 'Invierno',\n",
    "    9: 'Primavera', 10: 'Primavera', 11: 'Primavera'\n",
    "})\n",
    "\n",
    "colors = {'Verano': 'red', 'Oto√±o': 'orange', 'Invierno': 'blue', 'Primavera': 'green'}\n",
    "for season in df_analisis['season'].unique():\n",
    "    season_data = df_analisis[df_analisis['season'] == season]\n",
    "    ax4.scatter(season_data['TA_1_1_1'], season_data[variable_objetivo], \n",
    "               c=colors[season], alpha=0.6, label=season)\n",
    "\n",
    "ax4.set_title('üå°Ô∏è NEE vs Temperatura por Estaci√≥n', fontsize=12, fontweight='bold')\n",
    "ax4.set_xlabel('Temperatura (¬∞C)')\n",
    "ax4.set_ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "ax4.legend()\n",
    "ax4.axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de correlaciones por per√≠odo del d√≠a\n",
    "print(f\"\\nüîç AN√ÅLISIS DE CORRELACIONES POR PER√çODO DEL D√çA\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "# Definir per√≠odos del d√≠a\n",
    "df_analisis['periodo_dia'] = pd.cut(df_analisis['hour'], \n",
    "                                   bins=[0, 6, 12, 18, 24], \n",
    "                                   labels=['Madrugada', 'Ma√±ana', 'Tarde', 'Noche'])\n",
    "\n",
    "for periodo in df_analisis['periodo_dia'].unique():\n",
    "    if pd.notna(periodo):\n",
    "        periodo_data = df_analisis[df_analisis['periodo_dia'] == periodo]\n",
    "        correlaciones_periodo = periodo_data[variables_entrada + [variable_objetivo]].corr()\n",
    "        nee_corr = correlaciones_periodo[variable_objetivo].drop(variable_objetivo).sort_values(key=abs, ascending=False)\n",
    "        \n",
    "        print(f\"\\nüìä {periodo} ({len(periodo_data)} registros):\")\n",
    "        print(f\"Top 3 variables m√°s correlacionadas con NEE:\")\n",
    "        for i, (var, corr) in enumerate(nee_corr.head(3).items()):\n",
    "            print(f\"  {i+1}. {var}: {corr:.3f}\")\n",
    "\n",
    "# An√°lisis estacional\n",
    "print(f\"\\nüåç AN√ÅLISIS DE CORRELACIONES POR ESTACI√ìN\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "for season in df_analisis['season'].unique():\n",
    "    season_data = df_analisis[df_analisis['season'] == season]\n",
    "    correlaciones_season = season_data[variables_entrada + [variable_objetivo]].corr()\n",
    "    nee_corr = correlaciones_season[variable_objetivo].drop(variable_objetivo).sort_values(key=abs, ascending=False)\n",
    "    \n",
    "    print(f\"\\nüå± {season} ({len(season_data)} registros):\")\n",
    "    print(f\"Top 3 variables m√°s correlacionadas con NEE:\")\n",
    "    for i, (var, corr) in enumerate(nee_corr.head(3).items()):\n",
    "        print(f\"  {i+1}. {var}: {corr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb238f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# AN√ÅLISIS DE DISTRIBUCIONES Y OUTLIERS\n",
    "# ========================================\n",
    "\n",
    "# Crear figura para an√°lisis de distribuciones\n",
    "fig, axes = plt.subplots(4, 3, figsize=(20, 16))\n",
    "fig.suptitle('üìä DISTRIBUCIONES DE VARIABLES Y DETECCI√ìN DE OUTLIERS', fontsize=16, fontweight='bold')\n",
    "\n",
    "# An√°lisis de distribuciones y outliers\n",
    "outlier_summary = []\n",
    "\n",
    "for i, var in enumerate(variables_entrada + [variable_objetivo]):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    # Obtener datos de la variable\n",
    "    data = df_analisis[var].dropna()\n",
    "    \n",
    "    # Crear histograma\n",
    "    ax.hist(data, bins=50, alpha=0.7, color='skyblue', edgecolor='black', density=True)\n",
    "    ax.set_title(f'{var}', fontsize=10, fontweight='bold')\n",
    "    ax.set_xlabel('Valor')\n",
    "    if var == variable_objetivo:\n",
    "        ax.set_ylabel('toneladas CO‚ÇÇ/ha')\n",
    "    else:\n",
    "        ax.set_ylabel('Densidad')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Calcular estad√≠sticas de outliers usando IQR\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
    "    outlier_pct = (len(outliers) / len(data)) * 100\n",
    "    \n",
    "    # Agregar l√≠neas verticales para l√≠mites de outliers\n",
    "    ax.axvline(lower_bound, color='red', linestyle='--', alpha=0.8, linewidth=2, label='L√≠mite inferior')\n",
    "    ax.axvline(upper_bound, color='red', linestyle='--', alpha=0.8, linewidth=2, label='L√≠mite superior')\n",
    "    \n",
    "    # Agregar l√≠nea para la mediana\n",
    "    ax.axvline(data.median(), color='green', linestyle='-', alpha=0.8, linewidth=2, label='Mediana')\n",
    "    \n",
    "    # Agregar leyenda solo en el primer subplot\n",
    "    if i == 0:\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    # Guardar informaci√≥n de outliers\n",
    "    outlier_summary.append({\n",
    "        'Variable': var,\n",
    "        'Total_Outliers': len(outliers),\n",
    "        'Porcentaje_Outliers': round(outlier_pct, 2),\n",
    "        'Limite_Inferior': round(lower_bound, 3),\n",
    "        'Limite_Superior': round(upper_bound, 3),\n",
    "        'Min': round(data.min(), 3),\n",
    "        'Max': round(data.max(), 3),\n",
    "        'Media': round(data.mean(), 3),\n",
    "        'Mediana': round(data.median(), 3),\n",
    "        'Desviacion_Std': round(data.std(), 3)\n",
    "    })\n",
    "\n",
    "# Remover subplots vac√≠os si los hay\n",
    "for i in range(len(variables_entrada + [variable_objetivo]), 12):\n",
    "    row = i // 3\n",
    "    col = i % 3\n",
    "    if row < 4 and col < 3:\n",
    "        axes[row, col].remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar resumen de outliers\n",
    "print(f\"\\nüö® RESUMEN DE OUTLIERS Y ESTAD√çSTICAS\")\n",
    "print(f\"{'='*80}\")\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(outlier_df.to_string(index=False))\n",
    "\n",
    "# An√°lisis de valores extremos en NEE\n",
    "print(f\"\\n‚ö†Ô∏è  AN√ÅLISIS DE VALORES EXTREMOS EN NEE\")\n",
    "print(f\"{'='*40}\")\n",
    "nee_data = df_analisis[variable_objetivo]\n",
    "print(f\"Valores m√°s negativos (mayor absorci√≥n de CO‚ÇÇ):\")\n",
    "print(nee_data.nsmallest(5).to_string())\n",
    "print(f\"\\nValores m√°s positivos (mayor emisi√≥n de CO‚ÇÇ):\")\n",
    "print(nee_data.nlargest(5).to_string())\n",
    "\n",
    "# An√°lisis de condiciones extremas\n",
    "print(f\"\\nüå°Ô∏è  CONDICIONES DURANTE VALORES EXTREMOS DE NEE\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Condiciones durante m√°xima absorci√≥n\n",
    "max_absorption_idx = nee_data.idxmin()\n",
    "max_absorption_conditions = df_analisis.loc[max_absorption_idx, variables_entrada + ['TIMESTAMP_START']]\n",
    "print(f\"Condiciones durante m√°xima absorci√≥n (NEE = {nee_data.min():.3f}):\")\n",
    "print(f\"Fecha: {max_absorption_conditions['TIMESTAMP_START']}\")\n",
    "for var in variables_entrada:\n",
    "    print(f\"  {var}: {max_absorption_conditions[var]:.3f}\")\n",
    "\n",
    "# Condiciones durante m√°xima emisi√≥n\n",
    "max_emission_idx = nee_data.idxmax()\n",
    "max_emission_conditions = df_analisis.loc[max_emission_idx, variables_entrada + ['TIMESTAMP_START']]\n",
    "print(f\"\\nCondiciones durante m√°xima emisi√≥n (NEE = {nee_data.max():.3f}):\")\n",
    "print(f\"Fecha: {max_emission_conditions['TIMESTAMP_START']}\")\n",
    "for var in variables_entrada:\n",
    "    print(f\"  {var}: {max_emission_conditions[var]:.3f}\")\n",
    "\n",
    "# Crear boxplot para comparar variables\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15, 8))\n",
    "data_normalized = df_analisis[variables_entrada + [variable_objetivo]].copy()\n",
    "# Normalizar para comparar en el mismo gr√°fico\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_viz = StandardScaler()\n",
    "data_normalized[variables_entrada + [variable_objetivo]] = scaler_viz.fit_transform(data_normalized[variables_entrada + [variable_objetivo]])\n",
    "\n",
    "data_normalized.boxplot(ax=ax, rot=45, grid=True)\n",
    "ax.set_title('üì¶ COMPARACI√ìN DE DISTRIBUCIONES (DATOS NORMALIZADOS)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Valores normalizados (Z-score)')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20c92b",
   "metadata": {},
   "source": [
    "6. ‚öôÔ∏è Preparaci√≥n de los datos para entrenamiento\n",
    "\n",
    "Se dividen los datos en dos subconjuntos:\n",
    "\t‚Ä¢\tEntrenamiento: registros desde diciembre de 2016 hasta finales de 2017.\n",
    "\t‚Ä¢\tPrueba: registros del a√±o 2018, para evaluar la capacidad de generalizaci√≥n del modelo.\n",
    "\n",
    "Adicionalmente, se aplica una normalizaci√≥n tipo Z-score (media cero, desviaci√≥n unitaria) a las variables predictoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e161475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 6. Filtrar datos v√°lidos y robustecer preprocesamiento\n",
    "# ---------------------------\n",
    "\n",
    "def eliminar_outliers_iqr(df, variables):\n",
    "    df_filtrado = df.copy()\n",
    "    for var in variables:\n",
    "        Q1 = df_filtrado[var].quantile(0.25)\n",
    "        Q3 = df_filtrado[var].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower = Q1 - 1.5 * IQR\n",
    "        upper = Q3 + 1.5 * IQR\n",
    "        df_filtrado = df_filtrado[(df_filtrado[var] >= lower) & (df_filtrado[var] <= upper)]\n",
    "    return df_filtrado\n",
    "\n",
    "# 1. Eliminar outliers en variables de entrada y objetivo\n",
    "df_sin_outliers = eliminar_outliers_iqr(df, variables_entrada + [variable_objetivo])\n",
    "\n",
    "# 2. Eliminar filas con valores faltantes en variables seleccionadas\n",
    "df_modelo = df_sin_outliers.dropna(subset=variables_entrada + [variable_objetivo])\n",
    "\n",
    "# 3. (Ya no se filtra por periodo temporal aqu√≠)\n",
    "\n",
    "print(f\"Registros finales para modelado: {len(df_modelo):,}\")\n",
    "print(f\"Ventana temporal: {df_modelo['TIMESTAMP_START'].min()} - {df_modelo['TIMESTAMP_START'].max()}\")\n",
    "print(f\"A√±os incluidos: {sorted(df_modelo['year'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec40a471",
   "metadata": {},
   "source": [
    "El paso de ~13,000 registros a 6,497 se debe a la combinaci√≥n de varios filtros estrictos aplicados en el preprocesamiento, especialmente:\n",
    "\n",
    "1. **Eliminaci√≥n de outliers**:  \n",
    "   Se usa la funci√≥n `eliminar_outliers_iqr` para cada variable de entrada y la variable objetivo. Este m√©todo elimina cualquier fila donde alguna de estas variables est√© fuera del rango intercuart√≠lico (IQR). Si una fila es outlier en cualquier variable, se elimina. Esto ya puede reducir bastante el tama√±o.\n",
    "\n",
    "2. **Eliminaci√≥n de filas con NaN en cualquier variable relevante**:  \n",
    "   Despu√©s de quitar outliers, se eliminan todas las filas que tengan al menos un valor faltante (`NaN`) en cualquiera de las variables de entrada o en la variable objetivo.  \n",
    "   - Si tienes 11 variables de entrada + 1 objetivo = 12 columnas, y los NaN est√°n distribuidos (no siempre en la misma fila), la intersecci√≥n de filas completas puede ser mucho menor que el total de filas con alg√∫n NaN individual.\n",
    "   - Por ejemplo, si cada columna tiene un 10% de NaN, pero en diferentes filas, el resultado final puede ser mucho menos del 90% de los datos originales.\n",
    "\n",
    "3. **Filtros previos**:  \n",
    "   - Se eliminan filas con NDVI NaN al inicio.\n",
    "   - Se filtra el periodo temporal (desde diciembre 2016).\n",
    "   - Se reemplazan -9999 por NaN.\n",
    "\n",
    "**En resumen:**  \n",
    "La reducci√≥n dr√°stica ocurre porque:\n",
    "- Los NaN est√°n distribuidos en distintas columnas y filas, as√≠ que al exigir que todas las variables est√©n presentes en la misma fila, solo quedan las filas donde absolutamente todas las variables relevantes tienen datos observados y no son outlier.\n",
    "- Cada filtro (outlier, NaN, periodo, NDVI) va reduciendo el tama√±o, y el efecto acumulativo es fuerte.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260012b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 7. An√°lisis de variables de entrada y objetivo\n",
    "# ---------------------------\n",
    "\n",
    "print(\"\\nüìä Estad√≠sticas de variables de entrada:\\n\")\n",
    "print(df_modelo[variables_entrada].describe().transpose())\n",
    "\n",
    "print(\"\\nüìà Correlaci√≥n con la variable objetivo (NEE_PI_F):\\n\")\n",
    "correlaciones = df_modelo[variables_entrada + [variable_objetivo]].corr()\n",
    "print(correlaciones[[variable_objetivo]].sort_values(by=variable_objetivo, ascending=False))\n",
    "\n",
    "# Mapa de calor de correlaci√≥n\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlaciones, annot=True, cmap='coolwarm', fmt=\".2f\", square=True)\n",
    "plt.title(\"üìå Matriz de correlaci√≥n entre variables\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 8. Preparar datos para modelar\n",
    "# ---------------------------\n",
    "X = df_modelo[variables_entrada]\n",
    "y = df_modelo[variable_objetivo]\n",
    "\n",
    "# Normalizar\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Train/Test seg√∫n a√±o (entrenar con datos de 2016-2017, probar en 2018)\n",
    "X_train = X[df_modelo['year'] < 2018]\n",
    "X_test = X[df_modelo['year'] == 2018]\n",
    "y_train = y[df_modelo['year'] < 2018]\n",
    "y_test = y[df_modelo['year'] == 2018]\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ca4efc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec8f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 9. Entrenar modelo Random Forest\n",
    "# ---------------------------\n",
    "\n",
    "# ---------------------------\n",
    "# 9b. Optimizaci√≥n bayesiana de hiperpar√°metros para Random Forest\n",
    "# ---------------------------\n",
    "# Instalar scikit-optimize si no est√° instalado\n",
    "%pip install scikit-optimize\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Definir el espacio de b√∫squeda de hiperpar√°metros\n",
    "param_space = {\n",
    "    'n_estimators': Integer(50, 300),\n",
    "    'max_depth': Integer(3, 20),\n",
    "    'min_samples_split': Integer(2, 10),\n",
    "    'min_samples_leaf': Integer(1, 8),\n",
    "    'max_features': Real(0.5, 1.0, prior='uniform')\n",
    "}\n",
    "\n",
    "# Usar solo el conjunto de entrenamiento para la b√∫squeda\n",
    "cv = TimeSeriesSplit(n_splits=3)\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "opt = BayesSearchCV(\n",
    "    rf,\n",
    "    param_space,\n",
    "    n_iter=32,\n",
    "    cv=cv,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ")\n",
    "opt.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nMejores hiperpar√°metros encontrados:\")\n",
    "print(opt.best_params_)\n",
    "print(f\"Mejor score (neg RMSE): {opt.best_score_:.3f}\")\n",
    "\n",
    "# Usar el mejor modelo encontrado para el resto del flujo\n",
    "modelo = opt.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 9c. Comparaci√≥n de modelos con optimizaci√≥n bayesiana\n",
    "# ---------------------------\n",
    "%pip install scikit-optimize xgboost\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pandas as pd\n",
    "import numpy as np  # Asegura que np.sqrt est√© disponible\n",
    "\n",
    "modelos = {\n",
    "    'RandomForest': (RandomForestRegressor(random_state=42), {\n",
    "        'n_estimators': Integer(50, 300),\n",
    "        'max_depth': Integer(3, 20),\n",
    "        'min_samples_split': Integer(2, 10),\n",
    "        'min_samples_leaf': Integer(1, 8),\n",
    "        'max_features': Real(0.5, 1.0, prior='uniform')\n",
    "    }),\n",
    "    'XGBoost': (XGBRegressor(random_state=42, verbosity=0, n_jobs=-1), {\n",
    "        'n_estimators': Integer(50, 300),\n",
    "        'max_depth': Integer(3, 20),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'subsample': Real(0.5, 1.0),\n",
    "        'colsample_bytree': Real(0.5, 1.0)\n",
    "    }),\n",
    "    'GradientBoosting': (GradientBoostingRegressor(random_state=42), {\n",
    "        'n_estimators': Integer(50, 300),\n",
    "        'max_depth': Integer(3, 20),\n",
    "        'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "        'subsample': Real(0.5, 1.0)\n",
    "    }),\n",
    "    'SVR': (SVR(), {\n",
    "        'C': Real(0.1, 10.0, prior='log-uniform'),\n",
    "        'epsilon': Real(0.01, 1.0, prior='log-uniform'),\n",
    "        'kernel': Categorical(['rbf', 'linear'])\n",
    "    })\n",
    "}\n",
    "\n",
    "cv = TimeSeriesSplit(n_splits=3)\n",
    "resultados = []\n",
    "mejores_modelos = {}\n",
    "\n",
    "for nombre, (modelo, espacio) in modelos.items():\n",
    "    print(f\"\\nüîé Optimizando {nombre}...\")\n",
    "    opt = BayesSearchCV(\n",
    "        modelo,\n",
    "        espacio,\n",
    "        n_iter=24,\n",
    "        cv=cv,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "    opt.fit(X_train_scaled, y_train)\n",
    "    mejores_modelos[nombre] = opt.best_estimator_\n",
    "    y_train_pred = opt.best_estimator_.predict(X_train_scaled)\n",
    "    y_test_pred = opt.best_estimator_.predict(X_test_scaled)\n",
    "    # C√°lculo de RMSE corregido para compatibilidad\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    test_r2 = r2_score(y_test, y_test_pred)\n",
    "    resultados.append({\n",
    "        'Modelo': nombre,\n",
    "        'Best_Params': opt.best_params_,\n",
    "        'Train_RMSE': train_rmse,\n",
    "        'Test_RMSE': test_rmse,\n",
    "        'Train_R2': train_r2,\n",
    "        'Test_R2': test_r2,\n",
    "        'Overfitting_RMSE': train_rmse - test_rmse,\n",
    "        'Overfitting_R2': train_r2 - test_r2\n",
    "    })\n",
    "    print(f\"  Mejor RMSE validaci√≥n: {-opt.best_score_:.3f}\")\n",
    "    print(f\"  RMSE train: {train_rmse:.3f} | RMSE test: {test_rmse:.3f}\")\n",
    "    print(f\"  R2 train: {train_r2:.3f} | R2 test: {test_r2:.3f}\")\n",
    "    print(f\"  Overfitting (RMSE): {train_rmse - test_rmse:.3f}\")\n",
    "\n",
    "# Mostrar resumen comparativo\n",
    "df_resultados = pd.DataFrame(resultados)\n",
    "display(df_resultados[['Modelo', 'Train_RMSE', 'Test_RMSE', 'Train_R2', 'Test_R2', 'Overfitting_RMSE', 'Overfitting_R2']])\n",
    "\n",
    "# Seleccionar el mejor modelo seg√∫n RMSE test\n",
    "mejor_nombre = df_resultados.sort_values('Test_RMSE').iloc[0]['Modelo']\n",
    "modelo = mejores_modelos[mejor_nombre]\n",
    "print(f\"\\nüèÜ Mejor modelo seg√∫n RMSE test: {mejor_nombre}\")\n",
    "print(f\"Hiperpar√°metros: {df_resultados.loc[df_resultados['Modelo']==mejor_nombre, 'Best_Params'].values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc382e9f",
   "metadata": {},
   "source": [
    "En este paso, el modelo se entrena utilizando la mejor combinaci√≥n de hiperpar√°metros encontrada por la b√∫squeda bayesiana (`BayesSearchCV`). El flujo es el siguiente:\n",
    "\n",
    "1. **Optimizaci√≥n bayesiana**:  \n",
    "   Se exploran distintas combinaciones de hiperpar√°metros del `RandomForestRegressor` (como n√∫mero de √°rboles, profundidad m√°xima, tama√±o m√≠nimo de muestras, etc.) usando validaci√≥n cruzada sobre el conjunto de entrenamiento.  \n",
    "   El objetivo es minimizar el error cuadr√°tico medio (RMSE) negativo.\n",
    "\n",
    "2. **Selecci√≥n del mejor modelo**:  \n",
    "   Al finalizar la b√∫squeda, `opt.best_estimator_` contiene el modelo Random Forest entrenado con los mejores hiperpar√°metros encontrados.\n",
    "\n",
    "3. **Entrenamiento final**:  \n",
    "   El modelo √≥ptimo ya est√° entrenado con los datos de entrenamiento (`X_train_scaled`, `y_train`).  \n",
    "   No es necesario volver a entrenar el modelo manualmente despu√©s de la b√∫squeda, ya que `BayesSearchCV` lo hace autom√°ticamente.\n",
    "\n",
    "4. **Uso posterior**:  \n",
    "   El modelo (`modelo`) se utiliza directamente para predecir y evaluar sobre el conjunto de prueba (2018).\n",
    "\n",
    "En resumen:  \n",
    "Aqu√≠ ya tienes un modelo Random Forest entrenado y optimizado, listo para hacer predicciones y evaluar su desempe√±o. ¬øTe gustar√≠a una explicaci√≥n m√°s detallada de los hiperpar√°metros o del proceso de validaci√≥n cruzada?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7709b",
   "metadata": {},
   "source": [
    "**Entrenamiento del modelo:**\n",
    "\n",
    "El entrenamiento de los modelos de machine learning se realiz√≥ utilizando un enfoque robusto y realista para series de tiempo. Primero, los datos fueron cuidadosamente limpiados y preprocesados, eliminando outliers y registros con valores faltantes en las variables relevantes. Posteriormente, se dividieron en conjuntos de entrenamiento (a√±os 2016-2017) y prueba (a√±o 2018), asegurando as√≠ una validaci√≥n temporal adecuada y evitando el leakage de informaci√≥n del futuro al pasado. Para cada modelo (Random Forest, XGBoost, Gradient Boosting y SVR), se aplic√≥ una optimizaci√≥n bayesiana de hiperpar√°metros mediante validaci√≥n cruzada temporal (`TimeSeriesSplit`), buscando minimizar el error cuadr√°tico medio (RMSE) en validaci√≥n. El mejor modelo de cada tipo se seleccion√≥ autom√°ticamente y se evalu√≥ su desempe√±o sobre el conjunto de prueba, permitiendo una comparaci√≥n justa y rigurosa entre algoritmos y evitando el sobreajuste a periodos espec√≠ficos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52463e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 10. Evaluaci√≥n robusta y rigurosa del modelo\n",
    "# ---------------------------\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Predicci√≥n\n",
    "y_pred = modelo.predict(X_test_scaled)\n",
    "\n",
    "# M√©tricas de desempe√±o\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "medae = median_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\nüéØ Desempe√±o global del modelo en datos 2018:\")\n",
    "print(f\"   R¬≤: {r2:.3f}\")\n",
    "print(f\"   RMSE: {rmse:.3f} toneladas CO‚ÇÇ/ha\")\n",
    "print(f\"   MAE: {mae:.3f} toneladas CO‚ÇÇ/ha\")\n",
    "print(f\"   MedAE: {medae:.3f} toneladas CO‚ÇÇ/ha\")\n",
    "\n",
    "# An√°lisis de errores\n",
    "errores = y_test - y_pred\n",
    "\n",
    "print(\"\\nüìä Estad√≠sticas de los errores (residuales):\")\n",
    "print(errores.describe().round(3))\n",
    "\n",
    "# Gr√°fico de residuales\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(errores, bins=40, kde=True, color='purple')\n",
    "plt.title('Distribuci√≥n de errores (residuales)')\n",
    "plt.xlabel('Error (NEE observado - NEE predicho) [toneladas CO‚ÇÇ/ha]')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fico de dispersi√≥n predicci√≥n vs observaci√≥n\n",
    "plt.figure(figsize=(7, 7))\n",
    "plt.scatter(y_test, y_pred, alpha=0.3, edgecolors='k')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', label='Ideal')\n",
    "plt.xlabel(\"NEE observado [toneladas CO‚ÇÇ/ha]\")\n",
    "plt.ylabel(\"NEE predicho [toneladas CO‚ÇÇ/ha]\")\n",
    "plt.title(\"Predicci√≥n vs Observaci√≥n (2018)\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fico de errores vs predicci√≥n\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(y_pred, errores, alpha=0.3, edgecolors='k')\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel('NEE predicho [toneladas CO‚ÇÇ/ha]')\n",
    "plt.ylabel('Error (observado - predicho) [toneladas CO‚ÇÇ/ha]')\n",
    "plt.title('Error vs NEE predicho')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis temporal de errores\n",
    "if 'TIMESTAMP_START' in df_modelo.columns:\n",
    "    test_idx = df_modelo[df_modelo['year'] == 2018].index\n",
    "    fechas_test = df_modelo.loc[test_idx, 'TIMESTAMP_START']\n",
    "    plt.figure(figsize=(14, 4))\n",
    "    plt.plot(fechas_test, errores, '.', alpha=0.5)\n",
    "    plt.axhline(0, color='red', linestyle='--')\n",
    "    plt.title('Error temporal (2018)')\n",
    "    plt.xlabel('Fecha')\n",
    "    plt.ylabel('Error (observado - predicho) [toneladas CO‚ÇÇ/ha]')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluaci√≥n por subgrupos temporales (estaciones)\n",
    "if 'month' in df_modelo.columns:\n",
    "    estaciones = {12: 'Verano', 1: 'Verano', 2: 'Verano',\n",
    "                  3: 'Oto√±o', 4: 'Oto√±o', 5: 'Oto√±o',\n",
    "                  6: 'Invierno', 7: 'Invierno', 8: 'Invierno',\n",
    "                  9: 'Primavera', 10: 'Primavera', 11: 'Primavera'}\n",
    "    meses_test = df_modelo.loc[test_idx, 'month']\n",
    "    estaciones_test = meses_test.map(estaciones)\n",
    "    import pandas as pd\n",
    "    df_eval = pd.DataFrame({'obs': y_test, 'pred': y_pred, 'error': errores, 'estacion': estaciones_test})\n",
    "    print(\"\\nDesempe√±o por estaci√≥n (2018):\")\n",
    "    for est in df_eval['estacion'].unique():\n",
    "        sub = df_eval[df_eval['estacion'] == est]\n",
    "        print(f\"  {est}: R¬≤={r2_score(sub['obs'], sub['pred']):.3f}, RMSE={np.sqrt(mean_squared_error(sub['obs'], sub['pred'])):.3f} toneladas CO‚ÇÇ/ha, MAE={mean_absolute_error(sub['obs'], sub['pred']):.3f} toneladas CO‚ÇÇ/ha\")\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(x='estacion', y='error', data=df_eval, palette='Set2')\n",
    "    plt.title('Distribuci√≥n de errores por estaci√≥n (2018)')\n",
    "    plt.xlabel('Estaci√≥n')\n",
    "    plt.ylabel('Error (observado - predicho) [toneladas CO‚ÇÇ/ha]')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluaci√≥n por cuantiles de NEE observado\n",
    "cuantiles = pd.qcut(y_test, 4, labels=['Q1 (m√°s bajo)', 'Q2', 'Q3', 'Q4 (m√°s alto)'])\n",
    "df_eval['cuantil'] = cuantiles\n",
    "print(\"\\nDesempe√±o por cuartil de NEE observado (2018):\")\n",
    "for q in df_eval['cuantil'].unique():\n",
    "    sub = df_eval[df_eval['cuantil'] == q]\n",
    "    print(f\"  {q}: R¬≤={r2_score(sub['obs'], sub['pred']):.3f}, RMSE={np.sqrt(mean_squared_error(sub['obs'], sub['pred'])):.3f} toneladas CO‚ÇÇ/ha, MAE={mean_absolute_error(sub['obs'], sub['pred']):.3f} toneladas CO‚ÇÇ/ha\")\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.boxplot(x='cuantil', y='error', data=df_eval, palette='Set3')\n",
    "plt.title('Distribuci√≥n de errores por cuartil de NEE observado (2018)')\n",
    "plt.xlabel('Cuartil de NEE observado')\n",
    "plt.ylabel('Error (observado - predicho) [toneladas CO‚ÇÇ/ha]')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4882444",
   "metadata": {},
   "source": [
    "AN√ÅLISIS NEE (EMISI√ìN DE DI√ìXIDO DE CARBONO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bbefaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Promedios diarios, mensuales y anuales de NEE (base original) y gr√°ficos\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Asegurarse de que la columna de fecha es datetime\n",
    "if 'TIMESTAMP_START' in df.columns:\n",
    "    df['fecha'] = pd.to_datetime(df['TIMESTAMP_START'])\n",
    "else:\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "\n",
    "# 1. Promedio diario\n",
    "nee_diario = df.groupby(df['fecha'].dt.date)['NEE_PI_F'].mean()\n",
    "\n",
    "# 2. Promedio mensual\n",
    "nee_mensual = df.groupby(df['fecha'].dt.to_period('M'))['NEE_PI_F'].mean()\n",
    "\n",
    "# 3. Promedio anual\n",
    "nee_anual = df.groupby(df['fecha'].dt.year)['NEE_PI_F'].mean()\n",
    "\n",
    "# Graficar promedios diarios\n",
    "plt.figure(figsize=(14, 4))\n",
    "nee_diario.plot()\n",
    "plt.title('Promedio diario de NEE (base original)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graficar promedios mensuales\n",
    "plt.figure(figsize=(12, 4))\n",
    "nee_mensual.plot(marker='o')\n",
    "plt.title('Promedio mensual de NEE (base original)')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graficar promedios anuales\n",
    "plt.figure(figsize=(8, 4))\n",
    "nee_anual.plot(marker='o', color='green')\n",
    "plt.title('Promedio anual de NEE (base original)')\n",
    "plt.xlabel('A√±o')\n",
    "plt.ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar los valores calculados\n",
    "display(nee_diario.head())\n",
    "display(nee_mensual.head())\n",
    "display(nee_anual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b98a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Contraste NEE observado vs predicho en 2018\n",
    "# =============================\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Asegurarse de que existen las variables necesarias\n",
    "def get_fechas_test():\n",
    "    if 'fechas_test' in locals() or 'fechas_test' in globals():\n",
    "        return fechas_test\n",
    "    elif 'df_modelo' in locals() or 'df_modelo' in globals():\n",
    "        test_idx = df_modelo[df_modelo['year'] == 2018].index\n",
    "        return df_modelo.loc[test_idx, 'TIMESTAMP_START']\n",
    "    else:\n",
    "        raise ValueError('No se encontr√≥ la variable de fechas para el test.')\n",
    "\n",
    "fechas_test = get_fechas_test()\n",
    "\n",
    "# Crear DataFrame de comparaci√≥n\n",
    "comparacion = pd.DataFrame({\n",
    "    'fecha': pd.to_datetime(fechas_test).reset_index(drop=True),\n",
    "    'NEE_obs': y_test.reset_index(drop=True),\n",
    "    'NEE_pred': y_pred\n",
    "})\n",
    "\n",
    "# Promedios diarios\n",
    "nee_diario_obs = comparacion.groupby(comparacion['fecha'].dt.date)['NEE_obs'].mean()\n",
    "nee_diario_pred = comparacion.groupby(comparacion['fecha'].dt.date)['NEE_pred'].mean()\n",
    "\n",
    "# Promedios mensuales\n",
    "nee_mensual_obs = comparacion.groupby(comparacion['fecha'].dt.to_period('M'))['NEE_obs'].mean()\n",
    "nee_mensual_pred = comparacion.groupby(comparacion['fecha'].dt.to_period('M'))['NEE_pred'].mean()\n",
    "\n",
    "# Promedios anuales\n",
    "nee_anual_obs = comparacion.groupby(comparacion['fecha'].dt.year)['NEE_obs'].mean()\n",
    "nee_anual_pred = comparacion.groupby(comparacion['fecha'].dt.year)['NEE_pred'].mean()\n",
    "\n",
    "# Graficar promedios diarios\n",
    "plt.figure(figsize=(14, 4))\n",
    "nee_diario_obs.plot(label='Observado', color='black')\n",
    "nee_diario_pred.plot(label='Predicho', color='orange')\n",
    "plt.title('Promedio diario de NEE en 2018: Observado vs Predicho')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graficar promedios mensuales\n",
    "plt.figure(figsize=(10, 4))\n",
    "nee_mensual_obs.plot(marker='o', label='Observado', color='black')\n",
    "nee_mensual_pred.plot(marker='o', label='Predicho', color='orange')\n",
    "plt.title('Promedio mensual de NEE en 2018: Observado vs Predicho')\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Graficar promedios anuales\n",
    "plt.figure(figsize=(6, 4))\n",
    "nee_anual_obs.plot(marker='o', label='Observado', color='black')\n",
    "nee_anual_pred.plot(marker='o', label='Predicho', color='orange')\n",
    "plt.title('Promedio anual de NEE en 2018: Observado vs Predicho')\n",
    "plt.xlabel('A√±o')\n",
    "plt.ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Gr√°fica de barras para el promedio anual observado vs predicho\n",
    "plt.figure(figsize=(5, 5))\n",
    "bar_labels = ['Observado', 'Predicho']\n",
    "bar_values = [float(nee_anual_obs.values[-1]), float(nee_anual_pred.values[-1])]\n",
    "plt.bar(bar_labels, bar_values, color=['black', 'orange'])\n",
    "plt.title('Comparaci√≥n anual de NEE en 2018')\n",
    "plt.ylabel('NEE (toneladas CO‚ÇÇ/ha)')\n",
    "for i, v in enumerate(bar_values):\n",
    "    plt.text(i, v, f'{v:.2f}', ha='center', va='bottom', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar diferencias agregadas\n",
    "print('Diferencia promedio diaria (Obs - Pred):', (nee_diario_obs - nee_diario_pred).mean().round(3))\n",
    "print('Diferencia promedio mensual (Obs - Pred):', (nee_mensual_obs - nee_mensual_pred).mean().round(3))\n",
    "print('Diferencia promedio anual (Obs - Pred):', (nee_anual_obs - nee_anual_pred).mean().round(3))\n",
    "\n",
    "# Mostrar primeros valores\n",
    "print('\\nPrimeros valores diarios:')\n",
    "display(pd.DataFrame({'Obs': nee_diario_obs, 'Pred': nee_diario_pred}).head())\n",
    "print('\\nValores mensuales:')\n",
    "display(pd.DataFrame({'Obs': nee_mensual_obs, 'Pred': nee_mensual_pred}))\n",
    "print('\\nValores anuales:')\n",
    "display(pd.DataFrame({'Obs': nee_anual_obs, 'Pred': nee_anual_pred}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753d9481",
   "metadata": {},
   "source": [
    "ESTIMAR EL MODELO ML SOBRE CALDAS - COLOMBIA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c4a565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Descarga, procesamiento y predicci√≥n sobre Caldas, Colombia\n",
    "# =============================\n",
    "\n",
    "# NOTA: Este es un flujo ejemplo. Para producci√≥n, adapta credenciales y par√°metros seg√∫n tu acceso a APIs.\n",
    "\n",
    "# 1. Descarga de NDVI/PRI desde Sentinel-2 (usando Google Earth Engine o SentinelHub)\n",
    "# Aqu√≠ se muestra un ejemplo usando geemap y GEE para NDVI. Para producci√≥n, usa tu cuenta de GEE o descarga manual.\n",
    "\n",
    "import geemap\n",
    "import ee\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Inicializar GEE (requiere autenticaci√≥n la primera vez)\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except Exception as e:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# Definir regi√≥n de Caldas (ejemplo: bounding box)\n",
    "caldas_bbox = ee.Geometry.BBox(-76.2, 4.7, -75.0, 5.8)  # Ajusta seg√∫n necesidad\n",
    "\n",
    "# Definir periodo de inter√©s\n",
    "fecha_inicio = '2018-01-01'\n",
    "fecha_fin = '2018-12-31'\n",
    "\n",
    "# Cargar colecci√≥n Sentinel-2 y calcular NDVI\n",
    "s2 = ee.ImageCollection('COPERNICUS/S2_SR') \\\n",
    "    .filterBounds(caldas_bbox) \\\n",
    "    .filterDate(fecha_inicio, fecha_fin) \\\n",
    "    .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 20))\n",
    "\n",
    "def add_ndvi(image):\n",
    "    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')\n",
    "    return image.addBands(ndvi)\n",
    "\n",
    "s2_ndvi = s2.map(add_ndvi)\n",
    "\n",
    "# Reducir a valores medios diarios sobre la regi√≥n\n",
    "ndvi_diario = s2_ndvi.select('NDVI').mean().reduceRegion(\n",
    "    reducer=ee.Reducer.mean(),\n",
    "    geometry=caldas_bbox,\n",
    "    scale=20,\n",
    "    maxPixels=1e8\n",
    ")\n",
    "ndvi_valor = ndvi_diario.get('NDVI').getInfo()\n",
    "\n",
    "# 2. Descarga de meteorolog√≠a ERA5-Land (ejemplo usando archivo NetCDF ya descargado)\n",
    "import xarray as xr\n",
    "\n",
    "# Suponiendo que tienes el archivo NetCDF descargado en 'datos_netcdf/radiacion_solar_era5_2024_01.nc'\n",
    "# Para producci√≥n, automatiza la descarga con CDS API si es necesario\n",
    "nc_path = 'datos_netcdf/radiacion_solar_era5_2024_01.nc'\n",
    "ds = xr.open_dataset(nc_path)\n",
    "\n",
    "# Extraer variable de radiaci√≥n solar y promedio sobre la regi√≥n\n",
    "# Ajusta lat/lon seg√∫n Caldas\n",
    "lat_min, lat_max = 4.7, 5.8\n",
    "lon_min, lon_max = -76.2, -75.0\n",
    "\n",
    "ds_caldas = ds.sel(latitude=slice(lat_max, lat_min), longitude=slice(lon_min, lon_max))\n",
    "\n",
    "# Promedio temporal y espacial\n",
    "rad_solar = ds_caldas['ssrd'].mean(dim=['latitude', 'longitude']).to_dataframe().reset_index()\n",
    "\n",
    "# 3. Unir datos de NDVI y meteorolog√≠a (ejemplo simple)\n",
    "# Aqu√≠ solo se muestra NDVI y radiaci√≥n solar, pero debes agregar todas las variables requeridas por el modelo\n",
    "\n",
    "df_pred = pd.DataFrame({\n",
    "    'NDVI': [ndvi_valor],\n",
    "    'SW_IN_1_1_1': [rad_solar['ssrd'].mean() / (24*3600)],  # Convertir J/m2 a W/m2 si es necesario\n",
    "    # Agrega aqu√≠ el resto de variables meteorol√≥gicas y de teledetecci√≥n\n",
    "    # Usa valores promedio o temporales seg√∫n tu resoluci√≥n\n",
    "})\n",
    "\n",
    "# 4. Preprocesar y normalizar igual que en el entrenamiento\n",
    "X_pred = df_pred[variables_entrada]\n",
    "X_pred_scaled = scaler.transform(X_pred)\n",
    "\n",
    "# 5. Predecir NEE para Caldas\n",
    "nee_pred_caldas = modelo.predict(X_pred_scaled)\n",
    "print(f\"Predicci√≥n de NEE para Caldas (promedio periodo): {nee_pred_caldas[0]:.3f} toneladas CO‚ÇÇ/ha\")\n",
    "\n",
    "# NOTA: Para predicci√≥n diaria/mensual, repite el proceso para cada fecha y une los resultados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f08cda8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8286d80c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
